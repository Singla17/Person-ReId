{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COL780_proj.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GxbyehLobE7q",
        "1PexYAqr_PwR",
        "Cq4c6s_kbQON",
        "_XLOGZJhnxga",
        "hGRhlJ2jZXuc"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxbyehLobE7q"
      },
      "source": [
        "# Base Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuCu5-oNbIfh"
      },
      "source": [
        "LA Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_U-r2zgyej1M",
        "outputId": "7610e328-916f-47b2-bdf4-897339ceb489"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSSKYpWAa5ap"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "\n",
        "def weights_init_kaiming(m):\n",
        "    \"\"\"\n",
        "    Initialization of weights of the model layers \n",
        "    \"\"\"\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in') # For old pytorch, you may use kaiming_normal.\n",
        "    elif classname.find('Linear') != -1:\n",
        "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_out')\n",
        "        init.constant_(m.bias.data, 0.0)\n",
        "    elif classname.find('BatchNorm1d') != -1:\n",
        "        init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "def weights_init_classifier(m):\n",
        "    \"\"\"\n",
        "    Initialization of the classifier head wts of the model\n",
        "    \"\"\"\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Linear') != -1:\n",
        "        init.normal_(m.weight.data, std=0.001)\n",
        "        init.constant_(m.bias.data, 0.0)\n",
        "        \n",
        "class ClassBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    loclly aware network structure\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, class_num, droprate, relu=False, bnorm=True, num_bottleneck=512, linear=True, return_f = False):\n",
        "        super(ClassBlock, self).__init__()\n",
        "        self.return_f = return_f\n",
        "        add_block = []\n",
        "        if linear:\n",
        "            add_block += [nn.Linear(input_dim, num_bottleneck)]\n",
        "        else:\n",
        "            num_bottleneck = input_dim\n",
        "        if bnorm:\n",
        "            add_block += [nn.BatchNorm1d(num_bottleneck)]\n",
        "        if relu:\n",
        "            add_block += [nn.LeakyReLU(0.1)]\n",
        "        if droprate>0:\n",
        "            add_block += [nn.Dropout(p=droprate)]\n",
        "        add_block = nn.Sequential(*add_block)\n",
        "        add_block.apply(weights_init_kaiming)\n",
        "\n",
        "        classifier = []\n",
        "        classifier += [nn.Linear(num_bottleneck, class_num)]\n",
        "        classifier = nn.Sequential(*classifier)\n",
        "        classifier.apply(weights_init_classifier)\n",
        "\n",
        "        self.add_block = add_block\n",
        "        self.classifier = classifier\n",
        "    def forward(self, x):\n",
        "        x = self.add_block(x)\n",
        "        if self.return_f:\n",
        "            f = x\n",
        "            x = self.classifier(x)\n",
        "            return [x,f]\n",
        "        else:\n",
        "            x = self.classifier(x)\n",
        "            return x\n",
        "        \n",
        "class LATransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    The main model architecture\n",
        "    Here the \"model\" param in __init__ is the ViT backbone\n",
        "    \"\"\"\n",
        "    def __init__(self, model, lmbd, class_num,part,num_blocks,int_dim ):\n",
        "        super(LATransformer, self).__init__()\n",
        "        self.class_num = class_num\n",
        "        self.part = part # We cut the pool5 to sqrt(N) parts\n",
        "        self.num_blocks = num_blocks\n",
        "        self.model = model\n",
        "        self.model.head.requires_grad_ = False \n",
        "        self.cls_token = self.model.cls_token\n",
        "        self.pos_embed = self.model.pos_embed\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((self.part,int_dim))\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.lmbd = lmbd\n",
        "        for i in range(self.part):\n",
        "            name = 'classifier'+str(i)\n",
        "            setattr(self, name, ClassBlock(int_dim, self.class_num, droprate=0.5, relu=False, bnorm=True, num_bottleneck=256))\n",
        "\n",
        "        \n",
        "\n",
        "    def forward(self,x):\n",
        "        \n",
        "        # Divide input image into patch embeddings and add position embeddings\n",
        "        x = self.model.patch_embed(x)\n",
        "        cls_token = self.cls_token.expand(x.shape[0], -1, -1) \n",
        "        x = torch.cat((cls_token, x), dim=1)\n",
        "        x = self.model.pos_drop(x + self.pos_embed)\n",
        "        \n",
        "        # Feed forward through transformer blocks\n",
        "        for i in range(self.num_blocks):\n",
        "            x = self.model.blocks[i](x)\n",
        "        x = self.model.norm(x)\n",
        "        \n",
        "        # extract the cls token\n",
        "        cls_token_out = x[:, 0].unsqueeze(1)\n",
        "        \n",
        "        # Average pool\n",
        "        x = self.avgpool(x[:, 1:])\n",
        "        \n",
        "        # Add global cls token to each local token \n",
        "        for i in range(self.part):\n",
        "            out = torch.mul(x[:, i, :], self.lmbd)\n",
        "            x[:,i,:] = torch.div(torch.add(cls_token_out.squeeze(),out), 1+self.lmbd)\n",
        "        \n",
        "        # Locally aware network\n",
        "        part = {}\n",
        "        predict = {}\n",
        "        for i in range(self.part):\n",
        "            part[i] = x[:,i,:]\n",
        "            name = 'classifier'+str(i)\n",
        "            c = getattr(self,name)\n",
        "            predict[i] = c(part[i])\n",
        "        return predict\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PexYAqr_PwR"
      },
      "source": [
        "# Softmax Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cq4c6s_kbQON"
      },
      "source": [
        "###Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjfKahtzbUo8",
        "outputId": "e48cf58d-0e10-48ff-9e1b-d7b62bfdc6cc"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "\n",
        "!pip install timm\n",
        "import timm\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.7/dist-packages (0.4.12)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNOSZFYWbVar"
      },
      "source": [
        "class AverageMeter:\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gr5GeJ0YbanS"
      },
      "source": [
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    This method helps to seed the libraries, it is important to get reproducible results\n",
        "    \"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80oV01tXbd_6"
      },
      "source": [
        "def train_one_epoch(epoch, model, loader, optimizer, loss_fn,verbose=False):\n",
        "    \"\"\"\n",
        "    This method implements training of model for one epoch\n",
        "    \"\"\"\n",
        "    \n",
        "    batch_time_m = AverageMeter()\n",
        "    data_time_m = AverageMeter()\n",
        "\n",
        "    model.train()\n",
        "    epoch_accuracy = 0\n",
        "    epoch_loss = 0\n",
        "    end = time.time()\n",
        "\n",
        "    for index,(data, target) in enumerate(loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        data_time_m.update(time.time() - end)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        score = 0.0\n",
        "        sm = nn.Softmax(dim=1)\n",
        "        for k, v in output.items():\n",
        "            score += sm(output[k])\n",
        "        _, preds = torch.max(score.data, 1)\n",
        "        \n",
        "        loss = 0.0\n",
        "        for k,v in output.items():\n",
        "            loss += loss_fn(output[k], target)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_time_m.update(time.time() - end)\n",
        "        acc = (preds == target.data).float().mean()\n",
        "\n",
        "        epoch_loss += loss/len(loader)\n",
        "        epoch_accuracy += acc / len(loader)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"The loss at epoch \"+str(epoch)+ \" was \"+str(epoch_loss.data.item())+ \" and the training accuracy is \"+str(epoch_accuracy.data.item()))\n",
        "\n",
        "    return OrderedDict([('train_loss', epoch_loss.data.item()), (\"train_accuracy\", epoch_accuracy.data.item())])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpfIl3ssbijR"
      },
      "source": [
        "def visualization(loss_arr,epo,loss):\n",
        "  \"\"\"\n",
        "  This is to visualize the training curves\n",
        "  \"\"\"\n",
        "  x = np.linspace(1,epo,epo)\n",
        "  if loss:\n",
        "      plt.plot(x,loss_arr, label='Training Loss')\n",
        "      plt.ylabel('Loss')\n",
        "      plt.xlabel('Epochs')\n",
        "      plt.title('Training Curve')\n",
        "  else:\n",
        "      plt.plot(x,loss_arr, label='Training Accuracy')\n",
        "      plt.ylabel('Accuracy')\n",
        "      plt.xlabel('Epochs')\n",
        "      plt.title('Training Curve')\n",
        "  \n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dlfSLhrbmka"
      },
      "source": [
        "def training(model,optimizer,criterion,scheduler,num_epochs,verbose,blocks,unfreeze_after,train_loader):\n",
        "    \"\"\"\n",
        "    Simulates the training of model\n",
        "    \"\"\"\n",
        "    unfrozen_blocks = 0\n",
        "    train_loss=[]\n",
        "    train_accuracy=[]\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        if epoch%unfreeze_after==0:\n",
        "            unfrozen_blocks += 1\n",
        "            model = unfreeze_blocks(model,blocks, unfrozen_blocks)\n",
        "            optimizer.param_groups[0]['lr'] *= lr_decay \n",
        "            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "            print(\"Unfrozen Blocks: {}, Current lr: {}, Trainable Params: {}\".format(unfrozen_blocks, \n",
        "                                                                                 optimizer.param_groups[0]['lr'], \n",
        "                                                                                 trainable_params))\n",
        "    \n",
        "        train_metrics = train_one_epoch(epoch, model, train_loader, optimizer, criterion,verbose=verbose)\n",
        "        train_loss.append(train_metrics[\"train_loss\"])\n",
        "        train_accuracy.append(train_metrics[\"train_accuracy\"])\n",
        "    \n",
        "    visualization(train_loss, num_epochs, True)\n",
        "    visualization(train_accuracy, num_epochs, False)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUUeztQjbpe8"
      },
      "source": [
        "def freeze_all_blocks(model,blocks):\n",
        "    \"\"\"\n",
        "    This method is used to freeze all 12[as per the original publication] blocks of a ViT\n",
        "    \"\"\"\n",
        "    frozen_blocks = blocks\n",
        "    for block in model.model.blocks[:frozen_blocks]:\n",
        "        for param in block.parameters():\n",
        "            param.requires_grad=False\n",
        "            \n",
        "            \n",
        "def unfreeze_blocks(model, blocks, amount= 1):\n",
        "    \"\"\"\n",
        "    This method is used to unfreeze some blocks\n",
        "    \"\"\"\n",
        "    for block in model.model.blocks[(blocks-1)-amount:]:\n",
        "        for param in block.parameters():\n",
        "            param.requires_grad=True\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhzAzJs1btHR"
      },
      "source": [
        "inp_path =\"/content/drive/MyDrive/vision_project_data\"\n",
        "out_path = \"output.pth\"\n",
        "num_classes = 62"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-C03C3mjemDD",
        "outputId": "4e27fc38-ddef-4978-d52c-32fbee530ad7"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Using {}\".format(device))\n",
        "set_seed(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQpFXQshevn1"
      },
      "source": [
        "batch_size = 32\n",
        "num_epochs = 30\n",
        "lr = 3e-4\n",
        "gamma = 0.7\n",
        "unfreeze_after=2\n",
        "lr_decay=.8\n",
        "lmbd = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK-DdEn5e0zW"
      },
      "source": [
        "transform_train_list = [\n",
        "transforms.Resize((224,224), interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "transforms.RandomHorizontalFlip(),\n",
        "transforms.ToTensor(),\n",
        "transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "]\n",
        "    \n",
        "    \n",
        "data_transforms = {\n",
        "'train': transforms.Compose( transform_train_list )\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtYW_iJefYuN"
      },
      "source": [
        "image_datasets = {}\n",
        "image_datasets['train'] = datasets.ImageFolder(os.path.join(inp_path, 'train'),\n",
        "                                      data_transforms['train'])\n",
        "\n",
        "train_loader = DataLoader(dataset = image_datasets['train'], batch_size=batch_size, shuffle=True )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyvsMJ0mfdpo",
        "outputId": "27138134-1f4b-4f05-dfcb-12cd3706549f"
      },
      "source": [
        "\"\"\"\n",
        "Setting up model and training\n",
        "\"\"\"\n",
        "# Create LA Transformer\n",
        "vit_base = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=num_classes)\n",
        "vit_base= vit_base.to(device)\n",
        "vit_base.eval()\n",
        "num_la_blocks = 14\n",
        "blocks = 12\n",
        "int_dim = 768\n",
        "model = LATransformer(vit_base, lmbd,num_classes,num_la_blocks,blocks,int_dim).to(device)\n",
        "print(model.eval())\n",
        "\n",
        "# loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# optimizer\n",
        "optimizer = optim.Adam(model.parameters(),weight_decay=5e-4, lr=lr)\n",
        "\n",
        "# scheduler\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
        "freeze_all_blocks(model,blocks)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LATransformer(\n",
            "  (model): VisionTransformer(\n",
            "    (patch_embed): PatchEmbed(\n",
            "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "      (norm): Identity()\n",
            "    )\n",
            "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
            "    (blocks): Sequential(\n",
            "      (0): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (1): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (2): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (3): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (4): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (5): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (6): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (7): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (8): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (9): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (10): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (11): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "    (pre_logits): Identity()\n",
            "    (head): Linear(in_features=768, out_features=62, bias=True)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(14, 768))\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (classifier0): ClassBlock(\n",
            "    (add_block): Sequential(\n",
            "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
            "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): Dropout(p=0.5, inplace=False)\n",
            "    )\n",
            "    (classifier): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=62, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier1): ClassBlock(\n",
            "    (add_block): Sequential(\n",
            "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
            "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): Dropout(p=0.5, inplace=False)\n",
            "    )\n",
            "    (classifier): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=62, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier2): ClassBlock(\n",
            "    (add_block): Sequential(\n",
            "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
            "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): Dropout(p=0.5, inplace=False)\n",
            "    )\n",
            "    (classifier): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=62, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier3): ClassBlock(\n",
            "    (add_block): Sequential(\n",
            "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
            "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): Dropout(p=0.5, inplace=False)\n",
            "    )\n",
            "    (classifier): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=62, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier4): ClassBlock(\n",
            "    (add_block): Sequential(\n",
            "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
            "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): Dropout(p=0.5, inplace=False)\n",
            "    )\n",
            "    (classifier): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=62, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier5): ClassBlock(\n",
            "    (add_block): Sequential(\n",
            "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
            "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): Dropout(p=0.5, inplace=False)\n",
            "    )\n",
            "    (classifier): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=62, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier6): ClassBlock(\n",
            "    (add_block): Sequential(\n",
            "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
            "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): Dropout(p=0.5, inplace=False)\n",
            "    )\n",
            "    (classifier): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=62, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier7): ClassBlock(\n",
            "    (add_block): Sequential(\n",
            "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
            "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): Dropout(p=0.5, inplace=False)\n",
            "    )\n",
            "    (classifier): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=62, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier8): ClassBlock(\n",
            "    (add_block): Sequential(\n",
            "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
            "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): Dropout(p=0.5, inplace=False)\n",
            "    )\n",
            "    (classifier): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=62, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier9): ClassBlock(\n",
            "    (add_block): Sequential(\n",
            "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
            "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): Dropout(p=0.5, inplace=False)\n",
            "    )\n",
            "    (classifier): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=62, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier10): ClassBlock(\n",
            "    (add_block): Sequential(\n",
            "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
            "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): Dropout(p=0.5, inplace=False)\n",
            "    )\n",
            "    (classifier): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=62, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier11): ClassBlock(\n",
            "    (add_block): Sequential(\n",
            "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
            "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): Dropout(p=0.5, inplace=False)\n",
            "    )\n",
            "    (classifier): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=62, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier12): ClassBlock(\n",
            "    (add_block): Sequential(\n",
            "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
            "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): Dropout(p=0.5, inplace=False)\n",
            "    )\n",
            "    (classifier): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=62, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier13): ClassBlock(\n",
            "    (add_block): Sequential(\n",
            "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
            "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): Dropout(p=0.5, inplace=False)\n",
            "    )\n",
            "    (classifier): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=62, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GSKFXN9mfgKd",
        "outputId": "702f17d8-7b64-4cce-d553-754c7c32f22f"
      },
      "source": [
        "\"\"\"\n",
        "Training Begins\n",
        "\"\"\"\n",
        "\n",
        "print(\"Training Begins...\")\n",
        "model = training(model,optimizer,criterion,scheduler,num_epochs,True,blocks,unfreeze_after,train_loader)\n",
        "print(\"Training Completed\")\n",
        "torch.save(model.cpu().state_dict(), out_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Begins...\n",
            "Unfrozen Blocks: 1, Current lr: 0.00023999999999999998, Trainable Params: 17953954\n",
            "The loss at epoch 0 was 56.558860778808594 and the training accuracy is 0.2641128897666931\n",
            "The loss at epoch 1 was 50.52006149291992 and the training accuracy is 0.6250000596046448\n",
            "Unfrozen Blocks: 2, Current lr: 0.000192, Trainable Params: 25041826\n",
            "The loss at epoch 2 was 44.12189483642578 and the training accuracy is 0.7620967030525208\n",
            "The loss at epoch 3 was 38.5203857421875 and the training accuracy is 0.8518145084381104\n",
            "Unfrozen Blocks: 3, Current lr: 0.00015360000000000002, Trainable Params: 32129698\n",
            "The loss at epoch 4 was 33.60707473754883 and the training accuracy is 0.9133062362670898\n",
            "The loss at epoch 5 was 29.26893424987793 and the training accuracy is 0.9445563554763794\n",
            "Unfrozen Blocks: 4, Current lr: 0.00012288000000000002, Trainable Params: 39217570\n",
            "The loss at epoch 6 was 25.79910659790039 and the training accuracy is 0.9495967030525208\n",
            "The loss at epoch 7 was 22.561059951782227 and the training accuracy is 0.9637095332145691\n",
            "Unfrozen Blocks: 5, Current lr: 9.830400000000001e-05, Trainable Params: 46305442\n",
            "The loss at epoch 8 was 19.980636596679688 and the training accuracy is 0.9717739820480347\n",
            "The loss at epoch 9 was 17.819774627685547 and the training accuracy is 0.9818545579910278\n",
            "Unfrozen Blocks: 6, Current lr: 7.864320000000001e-05, Trainable Params: 53393314\n",
            "The loss at epoch 10 was 15.993633270263672 and the training accuracy is 0.9828627109527588\n",
            "The loss at epoch 11 was 14.366914749145508 and the training accuracy is 0.991935133934021\n",
            "Unfrozen Blocks: 7, Current lr: 6.291456000000001e-05, Trainable Params: 60481186\n",
            "The loss at epoch 12 was 13.084632873535156 and the training accuracy is 0.9989914894104004\n",
            "The loss at epoch 13 was 12.040534019470215 and the training accuracy is 0.9999995231628418\n",
            "Unfrozen Blocks: 8, Current lr: 5.0331648000000016e-05, Trainable Params: 67569058\n",
            "The loss at epoch 14 was 11.161625862121582 and the training accuracy is 0.9999995231628418\n",
            "The loss at epoch 15 was 10.418895721435547 and the training accuracy is 0.9999995231628418\n",
            "Unfrozen Blocks: 9, Current lr: 4.026531840000002e-05, Trainable Params: 74656930\n",
            "The loss at epoch 16 was 9.787375450134277 and the training accuracy is 0.9999995231628418\n",
            "The loss at epoch 17 was 9.22145938873291 and the training accuracy is 0.9999995231628418\n",
            "Unfrozen Blocks: 10, Current lr: 3.221225472000002e-05, Trainable Params: 81744802\n",
            "The loss at epoch 18 was 8.725656509399414 and the training accuracy is 0.9999995231628418\n",
            "The loss at epoch 19 was 8.444194793701172 and the training accuracy is 0.9999995231628418\n",
            "Unfrozen Blocks: 11, Current lr: 2.5769803776000016e-05, Trainable Params: 88832674\n",
            "The loss at epoch 20 was 8.01870346069336 and the training accuracy is 0.9999995231628418\n",
            "The loss at epoch 21 was 7.707145690917969 and the training accuracy is 0.9999995231628418\n",
            "Unfrozen Blocks: 12, Current lr: 2.0615843020800013e-05, Trainable Params: 88832674\n",
            "The loss at epoch 22 was 7.450028419494629 and the training accuracy is 0.9999995231628418\n",
            "The loss at epoch 23 was 7.236445426940918 and the training accuracy is 0.9999995231628418\n",
            "Unfrozen Blocks: 13, Current lr: 1.649267441664001e-05, Trainable Params: 88832674\n",
            "The loss at epoch 24 was 7.076934814453125 and the training accuracy is 0.9999995231628418\n",
            "The loss at epoch 25 was 6.876699924468994 and the training accuracy is 0.9999995231628418\n",
            "Unfrozen Blocks: 14, Current lr: 1.319413953331201e-05, Trainable Params: 88832674\n",
            "The loss at epoch 26 was 6.703747749328613 and the training accuracy is 0.9999995231628418\n",
            "The loss at epoch 27 was 6.601689338684082 and the training accuracy is 0.9999995231628418\n",
            "Unfrozen Blocks: 15, Current lr: 1.0555311626649608e-05, Trainable Params: 88832674\n",
            "The loss at epoch 28 was 6.436883926391602 and the training accuracy is 0.9999995231628418\n",
            "The loss at epoch 29 was 6.368015766143799 and the training accuracy is 0.9999995231628418\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XLOGZJhnxga"
      },
      "source": [
        "###Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0NPbF0QoDt4"
      },
      "source": [
        "def get_id(img_path):\n",
        "    camera_id = []\n",
        "    labels = []\n",
        "    for path, v in img_path:\n",
        "        label = path.split(\"/\")[-2]\n",
        "        filename = os.path.basename(path)\n",
        "        camera = filename.split('_')[0]\n",
        "        labels.append(int(label))\n",
        "        camera_id.append(int(camera))\n",
        "    return camera_id, labels"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHONeksonwi-"
      },
      "source": [
        "def extract_feature(model,dataloaders,device,image_dataset):\n",
        "    \"\"\"\n",
        "    Used to get the features/representation of queryset and galleryset\n",
        "    \"\"\"\n",
        "    features =  torch.FloatTensor()\n",
        "    count = 0\n",
        "    idx = 0\n",
        "    images= {}\n",
        "    for index,data in enumerate(dataloaders):\n",
        "        img, label = data    \n",
        "        img, label = img.to(device), label.to(device)\n",
        "\n",
        "        output = model(img) # (B, D, H, W) --> B: batch size, HxWxD: feature volume size\n",
        "\n",
        "        n, c, h, w = img.size()\n",
        "        \n",
        "        count += n\n",
        "        features = torch.cat((features, output.detach().cpu()), 0)\n",
        "        idx += 1\n",
        "        images[index]=image_dataset.imgs[index][0]\n",
        "    return features,images\n",
        "\n",
        "def search(index,query: str, k=1):\n",
        "    \"\"\"\n",
        "    to do a similarity based search using faiss library\n",
        "    \"\"\"\n",
        "    encoded_query = query.unsqueeze(dim=0).numpy()\n",
        "    top_k = index.search(encoded_query, k)\n",
        "    return top_k"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gB5omETln8CN"
      },
      "source": [
        "def rank1(label, output):\n",
        "    if label==output[1][0][0]:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def rank5(label, output):\n",
        "    if label in output[1][0][:5]:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def calc_ap(label, output):\n",
        "    count = 0\n",
        "    score = 0\n",
        "    good = 0\n",
        "    for out in output[1][0]:\n",
        "        count += 1\n",
        "        if out==label:\n",
        "            good += 1            \n",
        "            score += (good/count)\n",
        "    if good==0:\n",
        "        return 0\n",
        "    return score/good"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwRDcTvotwkT",
        "outputId": "0d4de77d-4156-4bea-cbea-336984f71a72"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Using {}\".format(device))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3DrA-JEoH4b",
        "outputId": "5f513269-1e9d-44cd-b437-3cc3955a83be"
      },
      "source": [
        "import os\n",
        "!pip install faiss-gpu\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "import argparse\n",
        "!pip install timm\n",
        "import timm\n",
        "import torch\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.7/dist-packages (1.7.1.post3)\n",
            "Collecting timm\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[K     || 376 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.19.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.4.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSQCA2RYoMhX"
      },
      "source": [
        "wts_path = \"/content/drive/MyDrive/vision_project_data/base_output.pth\"\n",
        "inp_path =\"/content/drive/MyDrive/vision_project_data/val\""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-Qc6EuC6jdm"
      },
      "source": [
        "class LATransformerTest(nn.Module):\n",
        "    def __init__(self, model, lmbd, class_num,part,num_blocks,int_dim ):\n",
        "        super(LATransformerTest, self).__init__()\n",
        "        \n",
        "        self.class_num = class_num\n",
        "        self.part = part # We cut the pool5 to sqrt(N) parts\n",
        "        self.num_blocks = num_blocks\n",
        "        self.model = model\n",
        "        self.model.head.requires_grad_ = False \n",
        "        self.cls_token = self.model.cls_token\n",
        "        self.pos_embed = self.model.pos_embed\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((self.part,int_dim))\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.lmbd = lmbd\n",
        "#         for i in range(self.part):\n",
        "#             name = 'classifier'+str(i)\n",
        "#             setattr(self, name, ClassBlock(768, self.class_num, droprate=0.5, relu=False, bnorm=True, num_bottleneck=256))\n",
        "\n",
        "        \n",
        "\n",
        "    def forward(self,x):\n",
        "        \n",
        "        # Divide input image into patch embeddings and add position embeddings\n",
        "        x = self.model.patch_embed(x)\n",
        "        cls_token = self.cls_token.expand(x.shape[0], -1, -1) \n",
        "        x = torch.cat((cls_token, x), dim=1)\n",
        "        x = self.model.pos_drop(x + self.pos_embed)\n",
        "        \n",
        "        # Feed forward through transformer blocks\n",
        "        for i in range(self.num_blocks):\n",
        "            x = self.model.blocks[i](x)\n",
        "        x = self.model.norm(x)\n",
        "        \n",
        "        # extract the cls token\n",
        "        cls_token_out = x[:, 0].unsqueeze(1)\n",
        "        \n",
        "        # Average pool\n",
        "        x = self.avgpool(x[:, 1:])\n",
        "        \n",
        "        # Add global cls token to each local token \n",
        "#         for i in range(self.part):\n",
        "#             out = torch.mul(x[:, i, :], self.lmbd)\n",
        "#             x[:,i,:] = torch.div(torch.add(cls_token_out.squeeze(),out), 1+self.lmbd)\n",
        "\n",
        "        return x.cpu()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNtTsxgZr7JA",
        "outputId": "5a3f85d2-84cf-4b57-ec4b-beddef2f19db"
      },
      "source": [
        "\"\"\"\n",
        "Load saved model\n",
        "\"\"\"\n",
        "num_classes = 62\n",
        "vit_base = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=num_classes)\n",
        "vit_base= vit_base.to(device)\n",
        "\n",
        "# Create La-Transformer\n",
        "\n",
        "num_la_blocks = 14\n",
        "blocks = 12\n",
        "int_dim = 768\n",
        "lmbd = 8\n",
        "num_classes = 62\n",
        "model = LATransformerTest(vit_base, lmbd,num_classes,num_la_blocks,blocks,int_dim).to(device)\n",
        "model.load_state_dict(torch.load(wts_path), strict=False)\n",
        "model.eval()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LATransformerTest(\n",
              "  (model): VisionTransformer(\n",
              "    (patch_embed): PatchEmbed(\n",
              "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "      (norm): Identity()\n",
              "    )\n",
              "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
              "    (blocks): Sequential(\n",
              "      (0): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "    (pre_logits): Identity()\n",
              "    (head): Linear(in_features=768, out_features=62, bias=True)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(14, 768))\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUoT9ujd4irt"
      },
      "source": [
        "batch_size = 1"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJMiqRquog-h"
      },
      "source": [
        "transform_query_list = [\n",
        "    transforms.Resize((224,224), interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "]\n",
        "transform_gallery_list = [\n",
        "        transforms.Resize(size=(224,224), interpolation=transforms.InterpolationMode.BICUBIC), #Image.BICUBIC\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]\n",
        "\n",
        "data_transforms = {\n",
        "        'query': transforms.Compose( transform_query_list ),\n",
        "        'gallery': transforms.Compose(transform_gallery_list),\n",
        "    }\n",
        "\n",
        "\n",
        "image_datasets = {}\n",
        "\n",
        "image_datasets['query'] = datasets.ImageFolder(os.path.join(inp_path, 'query'),\n",
        "                                          data_transforms['query'])\n",
        "image_datasets['gallery'] = datasets.ImageFolder(os.path.join(inp_path, 'gallery'),\n",
        "                                          data_transforms['gallery'])\n",
        "query_loader = DataLoader(dataset = image_datasets['query'], batch_size=batch_size, shuffle=False )\n",
        "gallery_loader = DataLoader(dataset = image_datasets['gallery'], batch_size=batch_size, shuffle=False)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "py94HElZ4Ymy"
      },
      "source": [
        "# Extract Query Features\n",
        "\n",
        "query_feature,query_images= extract_feature(model,query_loader,device,image_datasets['query'])\n",
        "\n",
        "# Extract Gallery Features\n",
        "\n",
        "gallery_feature,gallery_images = extract_feature(model,gallery_loader,device,image_datasets['gallery'])\n",
        "\n",
        "# Retrieve labels\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpzF3pkMOnnZ"
      },
      "source": [
        "gallery_path = image_datasets['gallery'].imgs\n",
        "query_path = image_datasets['query'].imgs\n",
        "\n",
        "gallery_cam,gallery_label = get_id(gallery_path)\n",
        "query_cam,query_label = get_id(query_path)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dln9GuZUrsq7"
      },
      "source": [
        "concatenated_query_vectors = []\n",
        "for idx,query in enumerate(query_feature):\n",
        "    fnorm = torch.norm(query, p=2, dim=1, keepdim=True)*np.sqrt(14)\n",
        "    query_norm = query.div(fnorm.expand_as(query))\n",
        "    concatenated_query_vectors.append(query_norm.view((-1)))\n",
        "\n",
        "concatenated_gallery_vectors = []\n",
        "for idx,gallery in enumerate(gallery_feature):\n",
        "    fnorm = torch.norm(gallery, p=2, dim=1, keepdim=True)*np.sqrt(14)\n",
        "    gallery_norm = gallery.div(fnorm.expand_as(gallery))\n",
        "    concatenated_gallery_vectors.append(gallery_norm.view((-1)))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_VbRP0K-shv"
      },
      "source": [
        "index = faiss.IndexIDMap(faiss.IndexFlatIP(10752))\n",
        "index.add_with_ids(np.array([t.numpy() for t in concatenated_gallery_vectors]),np.array(gallery_label).astype(np.int64))\n",
        "\n",
        "index2 = (faiss.IndexFlatIP((14*768)))\n",
        "index2.add(np.array([t.numpy() for t in concatenated_gallery_vectors]))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEjwqFSe3S4c"
      },
      "source": [
        "from shutil import copyfile\n",
        "import os\n",
        "vis_path = \"/content/drive/MyDrive/vision_project_data/visualisation\"\n",
        "mod_str = \"baseline\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nBTsrEGwow4",
        "outputId": "62e8694a-ac8b-4f04-ee6f-c995c894cb45"
      },
      "source": [
        "rank1_score = 0\n",
        "rank5_score = 0\n",
        "ap = 0\n",
        "itr_no = 0\n",
        "count = 0\n",
        "for query, label in zip(concatenated_query_vectors, query_label):\n",
        "    count +=1\n",
        "    label = label\n",
        "    output_met = search(index,query, k=10)\n",
        "\n",
        "    encoded_query = query.unsqueeze(dim=0).numpy()\n",
        "\n",
        "    output = index.search(encoded_query,10)\n",
        "    output2 = index2.search(encoded_query,10)\n",
        "    label_pred = output[1][0][0]\n",
        "\n",
        "    rank1_score += rank1(label, output_met) \n",
        "    rank5_score += rank5(label, output_met) \n",
        "    print(\"Correct: {}, Total: {}, Incorrect: {}\".format(rank1_score, count, count-rank1_score))\n",
        "    ap += calc_ap(label, output)\n",
        "\n",
        "    list_preds = output2[1]\n",
        "    pred_img = gallery_images[list_preds[0][0]]\n",
        "    pred_img2 = gallery_images[list_preds[0][1]]\n",
        "    pred_img3 = gallery_images[list_preds[0][2]]\n",
        "    pred_img4 = gallery_images[list_preds[0][3]]\n",
        "    pred_img5 = gallery_images[list_preds[0][4]]\n",
        "    pred_img6 = gallery_images[list_preds[0][5]]\n",
        "    pred_img7 = gallery_images[list_preds[0][6]]\n",
        "    pred_img8 = gallery_images[list_preds[0][7]]\n",
        "    pred_img9 = gallery_images[list_preds[0][8]]\n",
        "    pred_img10 = gallery_images[list_preds[0][9]]\n",
        "    inp_img = query_images[itr_no]\n",
        "    \n",
        "    if os.path.exists(vis_path+'/'+mod_str+'_'+str(count)):\n",
        "      shutil.rmtree(vis_path+'/'+mod_str+'_'+str(count))\n",
        "    os.mkdir(vis_path+'/'+mod_str+'_'+str(count))\n",
        "\n",
        "    copyfile(inp_img, vis_path+'/'+mod_str+'_'+str(count)+'/query'+'.png')\n",
        "    copyfile(pred_img, vis_path+'/'+mod_str+'_'+str(count)+'/gallery_'+'1'+'.png')\n",
        "    copyfile(pred_img2, vis_path+'/'+mod_str+'_'+str(count)+'/gallery_'+'2'+'.png')\n",
        "    copyfile(pred_img3, vis_path+'/'+mod_str+'_'+str(count)+'/gallery_'+'3'+'.png')\n",
        "    copyfile(pred_img4, vis_path+'/'+mod_str+'_'+str(count)+'/gallery_'+'4'+'.png')\n",
        "    copyfile(pred_img5, vis_path+'/'+mod_str+'_'+str(count)+'/gallery_'+'5'+'.png')\n",
        "    copyfile(pred_img6, vis_path+'/'+mod_str+'_'+str(count)+'/gallery_'+'6'+'.png')\n",
        "    copyfile(pred_img7, vis_path+'/'+mod_str+'_'+str(count)+'/gallery_'+'7'+'.png')\n",
        "    copyfile(pred_img8, vis_path+'/'+mod_str+'_'+str(count)+'/gallery_'+'8'+'.png')\n",
        "    copyfile(pred_img9, vis_path+'/'+mod_str+'_'+str(count)+'/gallery_'+'9'+'.png')\n",
        "    copyfile(pred_img10, vis_path+'/'+mod_str+'_'+str(count)+'/gallery_'+'10'+'.png')\n",
        "    itr_no +=1\n",
        "\n",
        "print(\"Rank1: %.3f, Rank5: %.3f, mAP: %.3f\"%(rank1_score/len(query_feature), \n",
        "                                              rank5_score/len(query_feature), \n",
        "                                              ap/len(query_feature)))  "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct: 1, Total: 1, Incorrect: 0\n",
            "Correct: 2, Total: 2, Incorrect: 0\n",
            "Correct: 3, Total: 3, Incorrect: 0\n",
            "Correct: 4, Total: 4, Incorrect: 0\n",
            "Correct: 5, Total: 5, Incorrect: 0\n",
            "Correct: 6, Total: 6, Incorrect: 0\n",
            "Correct: 7, Total: 7, Incorrect: 0\n",
            "Correct: 8, Total: 8, Incorrect: 0\n",
            "Correct: 9, Total: 9, Incorrect: 0\n",
            "Correct: 10, Total: 10, Incorrect: 0\n",
            "Correct: 11, Total: 11, Incorrect: 0\n",
            "Correct: 12, Total: 12, Incorrect: 0\n",
            "Correct: 13, Total: 13, Incorrect: 0\n",
            "Correct: 14, Total: 14, Incorrect: 0\n",
            "Correct: 15, Total: 15, Incorrect: 0\n",
            "Correct: 16, Total: 16, Incorrect: 0\n",
            "Correct: 17, Total: 17, Incorrect: 0\n",
            "Correct: 18, Total: 18, Incorrect: 0\n",
            "Correct: 19, Total: 19, Incorrect: 0\n",
            "Correct: 20, Total: 20, Incorrect: 0\n",
            "Correct: 21, Total: 21, Incorrect: 0\n",
            "Correct: 22, Total: 22, Incorrect: 0\n",
            "Correct: 22, Total: 23, Incorrect: 1\n",
            "Correct: 22, Total: 24, Incorrect: 2\n",
            "Correct: 23, Total: 25, Incorrect: 2\n",
            "Correct: 24, Total: 26, Incorrect: 2\n",
            "Correct: 25, Total: 27, Incorrect: 2\n",
            "Correct: 26, Total: 28, Incorrect: 2\n",
            "Rank1: 0.929, Rank5: 1.000, mAP: 0.908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGRhlJ2jZXuc"
      },
      "source": [
        "# Pooling(x+y) with AMS + Triplet Loss + Flip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CulRCE8TZXud"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "\n",
        "def weights_init_kaiming(m):\n",
        "    \"\"\"\n",
        "    Initialization of weights of the model layers \n",
        "    \"\"\"\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in') # For old pytorch, you may use kaiming_normal.\n",
        "    elif classname.find('Linear') != -1:\n",
        "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_out')\n",
        "        init.constant_(m.bias.data, 0.0)\n",
        "    elif classname.find('BatchNorm1d') != -1:\n",
        "        init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "def weights_init_classifier(m):\n",
        "    \"\"\"\n",
        "    Initialization of the classifier head wts of the model\n",
        "    \"\"\"\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Linear') != -1:\n",
        "        init.normal_(m.weight.data, std=0.001)\n",
        "        init.constant_(m.bias.data, 0.0)\n",
        "        \n",
        "class ClassBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    loclly aware network structure\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, class_num, droprate, relu=False, bnorm=True, num_bottleneck=512, linear=True, return_f = False):\n",
        "        super(ClassBlock, self).__init__()\n",
        "        self.return_f = return_f\n",
        "        add_block = []\n",
        "        if linear:\n",
        "            add_block += [nn.Linear(input_dim, num_bottleneck)]\n",
        "        else:\n",
        "            num_bottleneck = input_dim\n",
        "        if bnorm:\n",
        "            add_block += [nn.BatchNorm1d(num_bottleneck)]\n",
        "        if relu:\n",
        "            add_block += [nn.LeakyReLU(0.1)]\n",
        "        if droprate>0:\n",
        "            add_block += [nn.Dropout(p=droprate)]\n",
        "        add_block = nn.Sequential(*add_block)\n",
        "        add_block.apply(weights_init_kaiming)\n",
        "\n",
        "        classifier = []\n",
        "        classifier += [nn.Linear(num_bottleneck, class_num)]\n",
        "        classifier = nn.Sequential(*classifier)\n",
        "        classifier.apply(weights_init_classifier)\n",
        "\n",
        "        self.add_block = add_block\n",
        "        self.classifier = classifier\n",
        "    def forward(self, x):\n",
        "        x = self.add_block(x)\n",
        "        if self.return_f:\n",
        "            f = x\n",
        "            x = self.classifier(x)\n",
        "            return [x,f]\n",
        "        else:\n",
        "            x = self.classifier(x)\n",
        "            return x"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0o7_dAHZXue"
      },
      "source": [
        "class TripletLoss(nn.Module):\n",
        "    \"\"\"Triplet loss with hard positive/negative mining.\n",
        "    Reference:\n",
        "    Hermans et al. In Defense of the Triplet Loss for Person Re-Identification. arXiv:1703.07737.\n",
        "    Code imported from https://github.com/Cysu/open-reid/blob/master/reid/loss/triplet.py.\n",
        "    Args:\n",
        "        margin (float): margin for triplet.\n",
        "    \"\"\"\n",
        "    def __init__(self, margin=0.3, mutual_flag = False):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "        self.ranking_loss = nn.MarginRankingLoss(margin=margin)\n",
        "        self.mutual = mutual_flag\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs: feature matrix with shape (batch_size, feat_dim)\n",
        "            targets: ground truth labels with shape (num_classes)\n",
        "        \"\"\"\n",
        "        n = inputs.size(0)\n",
        "        # inputs = 1. * inputs / (torch.norm(inputs, 2, dim=-1, keepdim=True).expand_as(inputs) + 1e-12)\n",
        "        # Compute pairwise distance, replace by the official when merged\n",
        "        dist = torch.pow(inputs, 2).sum(dim=1, keepdim=True).expand(n, n)\n",
        "        dist = dist + dist.t()\n",
        "        dist.addmm_(1, -2, inputs, inputs.t())\n",
        "        dist = dist.clamp(min=1e-12).sqrt()  # for numerical stability\n",
        "        # For each anchor, find the hardest positive and negative\n",
        "        mask = targets.expand(n, n).eq(targets.expand(n, n).t())\n",
        "        dist_ap, dist_an = [], []\n",
        "        for i in range(n):\n",
        "            dist_ap.append(dist[i][mask[i]].max().unsqueeze(0))\n",
        "            dist_an.append(dist[i][mask[i] == 0].min().unsqueeze(0))\n",
        "        dist_ap = torch.cat(dist_ap)\n",
        "        dist_an = torch.cat(dist_an)\n",
        "        # Compute ranking hinge loss\n",
        "        y = torch.ones_like(dist_an)\n",
        "        loss = self.ranking_loss(dist_an, dist_ap, y)\n",
        "        if self.mutual:\n",
        "            return loss, dist\n",
        "        return loss"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4n2rkogZXuf"
      },
      "source": [
        "class LATransformer_AMSpoolsumtriplet(nn.Module):\n",
        "    \"\"\"\n",
        "    The main model architecture\n",
        "    Here the \"model\" param in __init__ is the ViT backbone\n",
        "    \"\"\"\n",
        "    def __init__(self, model, lmbd, class_num,part,num_blocks,int_dim ):\n",
        "        super(LATransformer_AMSpoolsumtriplet, self).__init__()\n",
        "        self.class_num = class_num\n",
        "        self.part = part # We cut the pool5 to sqrt(N) parts\n",
        "        self.num_blocks = num_blocks\n",
        "        self.model = model\n",
        "        self.model.head.requires_grad_ = False \n",
        "        self.cls_token = self.model.cls_token\n",
        "        self.pos_embed = self.model.pos_embed\n",
        "        self.avgpool2 = nn.AdaptiveAvgPool2d((self.part,int_dim))\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1,int_dim))\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.lmbd = lmbd\n",
        "        self.int_dim = int_dim\n",
        "        for i in range(self.part):\n",
        "            name = 'classifier'+str(i)\n",
        "            setattr(self, name, ClassBlock(int_dim, self.class_num, droprate=0.5, relu=False, bnorm=True, num_bottleneck=256,return_f=True))\n",
        "\n",
        "        \n",
        "\n",
        "    def forward(self,x):\n",
        "        \n",
        "        # Divide input image into patch embeddings and add position embeddings\n",
        "        x = self.model.patch_embed(x)\n",
        "        cls_token = self.cls_token.expand(x.shape[0], -1, -1) \n",
        "        x = torch.cat((cls_token, x), dim=1)\n",
        "        x = self.model.pos_drop(x + self.pos_embed)\n",
        "        \n",
        "        # Feed forward through transformer blocks\n",
        "        for i in range(self.num_blocks):\n",
        "            x = self.model.blocks[i](x)\n",
        "        x = self.model.norm(x)\n",
        "        \n",
        "        # extract the cls token\n",
        "        cls_token_out = x[:, 0].unsqueeze(1)\n",
        "        \n",
        "        # Average pool\n",
        "        x2= x.detach().clone()\n",
        "        x2 = self.avgpool2(x2)\n",
        "        x = torch.reshape(x[:,1:],(x.shape[0],self.part,self.part,self.int_dim))\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.reshape(x,(x.shape[0],self.part,self.int_dim))\n",
        "        x = (x+x2)/2\n",
        "        \n",
        "        # Add global cls token to each local token \n",
        "        for i in range(self.part):\n",
        "            out = torch.mul(x[:, i, :], self.lmbd)\n",
        "            x[:,i,:] = torch.div(torch.add(cls_token_out.squeeze(),out), 1+self.lmbd)\n",
        "        \n",
        "        # Locally aware network\n",
        "        part = {}\n",
        "        predict = {}\n",
        "        pred = {}\n",
        "        pred_classes = {}\n",
        "        for i in range(self.part):\n",
        "            part[i] = x[:,i,:]\n",
        "            name = 'classifier'+str(i)\n",
        "            c = getattr(self,name)\n",
        "            temp = c(part[i])\n",
        "            predict[i] = temp[1]\n",
        "            pred[i] = temp[0]\n",
        "            pred_classes[i] = temp[0]\n",
        "        return (pred,predict,pred_classes)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "752Wb29nZXug"
      },
      "source": [
        "class AM_Softmax(nn.Module): #requires classification layer for normalization \n",
        "    def __init__(self, m=0.35, s=30, d=2048, num_classes=625, use_gpu=True , epsilon=0.1,smoothing=True):\n",
        "        super(AM_Softmax, self).__init__()\n",
        "        self.m = m\n",
        "        self.s = s \n",
        "        self.num_classes = num_classes\n",
        "        self.CrossEntropy = CrossEntropyLabelSmooth(self.num_classes , use_gpu=use_gpu)\n",
        "        self.smoothing = smoothing\n",
        "\n",
        "    def forward(self, features, labels , classifier  ):\n",
        "        '''\n",
        "        x : feature vector : (b x  d) b= batch size d = dimension \n",
        "        labels : (b,)\n",
        "        classifier : Fully Connected weights of classification layer (dxC), C is the number of classes: represents the vectors for class, assumed to be an object of nn.sequential\n",
        "        '''\n",
        "        features = nn.functional.normalize(features, p=2, dim=1) # normalize the features\n",
        "        with torch.no_grad():\n",
        "            classifier[0].weight=nn.Parameter(classifier[0].weight.div(torch.norm(classifier[0].weight, dim=1, keepdim=True)))  ## [0] bracing as assumed to be from nn.sequential\n",
        "\n",
        "        cos_angle = classifier(features)\n",
        "        cos_angle = torch.clamp( cos_angle , min = -1 , max = 1 ) \n",
        "        b = features.size(0)\n",
        "        for i in range(b):\n",
        "            cos_angle[i][labels[i]] = cos_angle[i][labels[i]]  - self.m \n",
        "        weighted_cos_angle = self.s * cos_angle\n",
        "        log_probs = self.CrossEntropy(weighted_cos_angle , labels, use_label_smoothing=self.smoothing)\n",
        "        return log_probs"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfGAnrKaZXuh"
      },
      "source": [
        "class CrossEntropyLabelSmooth(nn.Module):\n",
        "    \"\"\"Cross entropy loss with label smoothing regularizer.\n",
        "    Reference:\n",
        "    Szegedy et al. Rethinking the Inception Architecture for Computer Vision. CVPR 2016.\n",
        "    Equation: y = (1 - epsilon) * y + epsilon / K.\n",
        "    Args:\n",
        "        num_classes (int): number of classes.\n",
        "        epsilon (float): weight.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, epsilon=0.1, use_gpu=True):\n",
        "        super(CrossEntropyLabelSmooth, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.epsilon = epsilon\n",
        "        self.use_gpu = use_gpu\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, inputs, targets, use_label_smoothing=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs: prediction matrix (before softmax) with shape (batch_size, num_classes)\n",
        "            targets: ground truth labels with shape (batch_size,)\n",
        "        \"\"\"\n",
        "        log_probs = self.logsoftmax(inputs)\n",
        "        targets = torch.zeros(log_probs.size()).scatter_(1, targets.unsqueeze(1).data.cpu(), 1)\n",
        "        if self.use_gpu: targets = targets.to(torch.device('cuda'))\n",
        "        if use_label_smoothing:\n",
        "            targets = (1 - self.epsilon) * targets + self.epsilon / self.num_classes\n",
        "        loss = (- targets * log_probs).mean(0).sum()\n",
        "        return loss"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSE2-4y0ZXuh"
      },
      "source": [
        "###Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQBfBZZhZXui",
        "outputId": "8a257155-33e3-4ed6-972a-aafe0c663f76"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "\n",
        "!pip install timm\n",
        "import timm\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[?25l\r\u001b[K     |                               | 10 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |                              | 20 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |                             | 30 kB 27.9 MB/s eta 0:00:01\r\u001b[K     |                            | 40 kB 21.6 MB/s eta 0:00:01\r\u001b[K     |                           | 51 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |                          | 61 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |                          | 71 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |                         | 81 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |                        | 92 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |                       | 102 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |                      | 112 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |                     | 122 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |                    | 133 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |                   | 143 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |                   | 153 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |                  | 163 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |                 | 174 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |                | 184 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |               | 194 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |              | 204 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |             | 215 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |            | 225 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |            | 235 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |           | 245 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |          | 256 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |         | 266 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |        | 276 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |       | 286 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |      | 296 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |      | 307 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |     | 317 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |    | 327 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |   | 337 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |  | 348 kB 10.0 MB/s eta 0:00:01\r\u001b[K     | | 358 kB 10.0 MB/s eta 0:00:01\r\u001b[K     || 368 kB 10.0 MB/s eta 0:00:01\r\u001b[K     || 376 kB 10.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.19.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.4.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-Y4vQnsZXuj"
      },
      "source": [
        "class AverageMeter:\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejrsWLazZXuj"
      },
      "source": [
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    This method helps to seed the libraries, it is important to get reproducible results\n",
        "    \"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zZr8fhHZXuk"
      },
      "source": [
        "def train_one_epoch_AMS_triplet(epoch, model, loader, optimizer, loss_fn,loss2,verbose=False):\n",
        "    \"\"\"\n",
        "    This method implements training of model for one epoch with AM Softmax loss function\n",
        "    \"\"\"\n",
        "    \n",
        "    batch_time_m = AverageMeter()\n",
        "    data_time_m = AverageMeter()\n",
        "\n",
        "    model.train()\n",
        "    epoch_accuracy = 0\n",
        "    epoch_loss = 0\n",
        "    end = time.time()\n",
        "\n",
        "    for index,(data, target) in enumerate(loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        data_time_m.update(time.time() - end)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        (pred,output,classes) = model(data)\n",
        "        score = 0.0\n",
        "        sm = nn.Softmax(dim=1)\n",
        "        for k, v in classes.items():\n",
        "            score += sm(classes[k])\n",
        "        _, preds = torch.max(score.data, 1)\n",
        "        \n",
        "        loss = 0.0\n",
        "        for k,v in output.items():\n",
        "            name = 'classifier'+str(k)\n",
        "            c = getattr(model,name)\n",
        "            c = getattr(c,'classifier')\n",
        "            loss += loss_fn(output[k], target,c)\n",
        "            loss += loss2(pred[k],target)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_time_m.update(time.time() - end)\n",
        "        acc = (preds == target.data).float().mean()\n",
        "\n",
        "        epoch_loss += loss/len(loader)\n",
        "        epoch_accuracy += acc / len(loader)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"The loss at epoch \"+str(epoch)+ \" was \"+str(epoch_loss.data.item())+ \" and the training accuracy is \"+str(epoch_accuracy.data.item()))\n",
        "\n",
        "    return OrderedDict([('train_loss', epoch_loss.data.item()), (\"train_accuracy\", epoch_accuracy.data.item())])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba59M3wGZXul"
      },
      "source": [
        "def visualization(loss_arr,epo,loss):\n",
        "  \"\"\"\n",
        "  This is to visualize the training curves\n",
        "  \"\"\"\n",
        "  x = np.linspace(1,epo,epo)\n",
        "  if loss:\n",
        "      plt.plot(x,loss_arr, label='Training Loss')\n",
        "      plt.ylabel('Loss')\n",
        "      plt.xlabel('Epochs')\n",
        "      plt.title('Training Curve')\n",
        "  else:\n",
        "      plt.plot(x,loss_arr, label='Training Accuracy')\n",
        "      plt.ylabel('Accuracy')\n",
        "      plt.xlabel('Epochs')\n",
        "      plt.title('Training Curve')\n",
        "  \n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Y9qdsAeZXul"
      },
      "source": [
        "def training_AMS_triplet(model,optimizer,criterion,scheduler,num_epochs,verbose,blocks,unfreeze_after,train_loader,loss2):\n",
        "    \"\"\"\n",
        "    Simulates the training of model\n",
        "    \"\"\"\n",
        "    unfrozen_blocks = 0\n",
        "    train_loss=[]\n",
        "    train_accuracy=[]\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        if epoch%unfreeze_after==0:\n",
        "            unfrozen_blocks += 1\n",
        "            model = unfreeze_blocks(model,blocks, unfrozen_blocks)\n",
        "            optimizer.param_groups[0]['lr'] *= lr_decay \n",
        "            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "            print(\"Unfrozen Blocks: {}, Current lr: {}, Trainable Params: {}\".format(unfrozen_blocks, \n",
        "                                                                                 optimizer.param_groups[0]['lr'], \n",
        "                                                                                 trainable_params))\n",
        "    \n",
        "        train_metrics = train_one_epoch_AMS_triplet(epoch, model, train_loader, optimizer, criterion,loss2,verbose=verbose)\n",
        "        train_loss.append(train_metrics[\"train_loss\"])\n",
        "        train_accuracy.append(train_metrics[\"train_accuracy\"])\n",
        "    \n",
        "    visualization(train_loss, num_epochs, True)\n",
        "    visualization(train_accuracy, num_epochs, False)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqdky5RdZXum"
      },
      "source": [
        "def freeze_all_blocks(model,blocks):\n",
        "    \"\"\"\n",
        "    This method is used to freeze all 12[as per the original publication] blocks of a ViT\n",
        "    \"\"\"\n",
        "    frozen_blocks = blocks\n",
        "    for block in model.model.blocks[:frozen_blocks]:\n",
        "        for param in block.parameters():\n",
        "            param.requires_grad=False\n",
        "            \n",
        "            \n",
        "def unfreeze_blocks(model, blocks, amount= 1):\n",
        "    \"\"\"\n",
        "    This method is used to unfreeze some blocks\n",
        "    \"\"\"\n",
        "    for block in model.model.blocks[(blocks-1)-amount:]:\n",
        "        for param in block.parameters():\n",
        "            param.requires_grad=True\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJbTwZITZXun"
      },
      "source": [
        "inp_path =\"/content/drive/MyDrive/vision_project_data\"\n",
        "out_path = \"/content/drive/MyDrive/vision_project_data/ams_pool_sum_triplet_output.pth\"\n",
        "num_classes = 62"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGV4Sxk5ZXun",
        "outputId": "ef0194e8-0f15-4202-a19b-191e89e018e5"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Using {}\".format(device))\n",
        "set_seed(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f1gLXCFZXuo"
      },
      "source": [
        "batch_size = 32\n",
        "num_epochs = 30\n",
        "lr = 3e-4\n",
        "gamma = 0.7\n",
        "unfreeze_after=2\n",
        "lr_decay=.8\n",
        "lmbd = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tdqh2wziZXup"
      },
      "source": [
        "transform_train_list = [\n",
        "transforms.Resize((224,224), interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "transforms.RandomHorizontalFlip(),\n",
        "transforms.ToTensor(),\n",
        "transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "]\n",
        "    \n",
        "    \n",
        "data_transforms = {\n",
        "'train': transforms.Compose( transform_train_list )\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZ7MRUCVZXup"
      },
      "source": [
        "image_datasets = {}\n",
        "image_datasets['train'] = datasets.ImageFolder(os.path.join(inp_path, 'train'),\n",
        "                                      data_transforms['train'])\n",
        "\n",
        "train_loader = DataLoader(dataset = image_datasets['train'], batch_size=batch_size, shuffle=True )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NH62e_TRZXuq",
        "outputId": "e5689e57-76f4-4190-8e9d-e9f5ae8f6056"
      },
      "source": [
        "\"\"\"\n",
        "Setting up model and training\n",
        "\"\"\"\n",
        "# Create LA Transformer\n",
        "vit_base = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=num_classes)\n",
        "vit_base= vit_base.to(device)\n",
        "vit_base.eval()\n",
        "num_la_blocks = 14\n",
        "blocks = 12\n",
        "int_dim = 768\n",
        "model = LATransformer_AMSpoolsumtriplet(vit_base, lmbd,num_classes,num_la_blocks,blocks,int_dim).to(device)\n",
        "print(model.eval())\n",
        "\n",
        "# loss function\n",
        "criterion = AM_Softmax(m=0.3,s=15,d=256,num_classes=num_classes,use_gpu=True,epsilon=0.1,smoothing=False)\n",
        "loss2 = TripletLoss()\n",
        "\n",
        "# optimizer\n",
        "optimizer = optim.Adam(model.parameters(),weight_decay=5e-4, lr=lr)\n",
        "\n",
        "# scheduler\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
        "freeze_all_blocks(model,blocks)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LATransformer_AMSpoolsumtriplet(\n",
            "  (model): VisionTransformer(\n",
            "    (patch_embed): PatchEmbed(\n",
            "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "      (norm): Identity()\n",
            "    )\n",
            "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
            "    (blocks): Sequential(\n",
            "      (0): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (1): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (2): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (3): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (4): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (5): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (6): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (7): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (8): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (9): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (10): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (11): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "    (pre_logits): Identity()\n",
            "    (head): Linear(in_features=768, out_features=62, bias=True)\n",
            "  )\n",
            "  (avgpool2): AdaptiveAvgPool2d(output_size=(14, 768))\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 768))\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (classifier0): ClassBlock(\n",
            "    (add_block): Sequential(\n",
            "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
            "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): Dropout(p=0.5, inplace=False)\n",
            "    )\n",
            "    (classifier): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=62, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier1): ClassBlock(\n",
            "    (add_block): Sequential(\n",
            "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
            "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): Dropout(p=0.5, inplace=False)\n",
            "    )\n",
            "    (classifier): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=62, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier2): ClassBlock(\n",
            "    (add_block): Sequential(\n",
            "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
            "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): Dropout(p=0.5, inplace=False)\n",
            "    )\n",
            "    (classifier): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=62, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier3): ClassBlock(\n",
            "    (add_block): Sequential(\n",
            "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
            "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): Dropout(p=0.5, inplace=False)\n",
            "    )\n",
            "    (classifier): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=62, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier4): ClassBlock(\n",
            "    (add_block): Sequential(\n",
            "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
            "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): Dropout(p=0.5, inplace=False)\n",
            "    )\n",
            "    (classifier): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=62, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier5): ClassBlock(\n",
            "    (add_block): Sequential(\n",
            "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
            "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): Dropout(p=0.5, inplace=False)\n",
            "    )\n",
            "    (classifier): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=62, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier6): ClassBlock(\n",
            "    (add_block): Sequential(\n",
            "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
            "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): Dropout(p=0.5, inplace=False)\n",
            "    )\n",
            "    (classifier): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=62, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier7): ClassBlock(\n",
            "    (add_block): Sequential(\n",
            "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
            "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): Dropout(p=0.5, inplace=False)\n",
            "    )\n",
            "    (classifier): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=62, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier8): ClassBlock(\n",
            "    (add_block): Sequential(\n",
            "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
            "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): Dropout(p=0.5, inplace=False)\n",
            "    )\n",
            "    (classifier): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=62, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier9): ClassBlock(\n",
            "    (add_block): Sequential(\n",
            "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
            "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): Dropout(p=0.5, inplace=False)\n",
            "    )\n",
            "    (classifier): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=62, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier10): ClassBlock(\n",
            "    (add_block): Sequential(\n",
            "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
            "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): Dropout(p=0.5, inplace=False)\n",
            "    )\n",
            "    (classifier): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=62, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier11): ClassBlock(\n",
            "    (add_block): Sequential(\n",
            "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
            "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): Dropout(p=0.5, inplace=False)\n",
            "    )\n",
            "    (classifier): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=62, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier12): ClassBlock(\n",
            "    (add_block): Sequential(\n",
            "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
            "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): Dropout(p=0.5, inplace=False)\n",
            "    )\n",
            "    (classifier): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=62, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier13): ClassBlock(\n",
            "    (add_block): Sequential(\n",
            "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
            "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): Dropout(p=0.5, inplace=False)\n",
            "    )\n",
            "    (classifier): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=62, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8RZTbv99ZXur",
        "outputId": "5d8d4320-744e-4467-f614-7c5fb5b92490"
      },
      "source": [
        "\"\"\"\n",
        "Training Begins\n",
        "\"\"\"\n",
        "\n",
        "print(\"Training Begins...\")\n",
        "model = training_AMS_triplet(model,optimizer,criterion,scheduler,num_epochs,True,blocks,unfreeze_after,train_loader,loss2)\n",
        "print(\"Training Completed\")\n",
        "torch.save(model.cpu().state_dict(), out_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Begins...\n",
            "Unfrozen Blocks: 1, Current lr: 0.00023999999999999998, Trainable Params: 17953954\n",
            "The loss at epoch 0 was 138.71859741210938 and the training accuracy is 0.05040322244167328\n",
            "The loss at epoch 1 was 123.28632354736328 and the training accuracy is 0.3135080933570862\n",
            "Unfrozen Blocks: 2, Current lr: 0.000192, Trainable Params: 25041826\n",
            "The loss at epoch 2 was 112.0887222290039 and the training accuracy is 0.6018145680427551\n",
            "The loss at epoch 3 was 100.70662689208984 and the training accuracy is 0.7822580337524414\n",
            "Unfrozen Blocks: 3, Current lr: 0.00015360000000000002, Trainable Params: 32129698\n",
            "The loss at epoch 4 was 90.1582260131836 and the training accuracy is 0.893144965171814\n",
            "The loss at epoch 5 was 77.62940979003906 and the training accuracy is 0.9566529989242554\n",
            "Unfrozen Blocks: 4, Current lr: 0.00012288000000000002, Trainable Params: 39217570\n",
            "The loss at epoch 6 was 67.12792205810547 and the training accuracy is 0.9798386096954346\n",
            "The loss at epoch 7 was 57.321311950683594 and the training accuracy is 0.997983455657959\n",
            "Unfrozen Blocks: 5, Current lr: 9.830400000000001e-05, Trainable Params: 46305442\n",
            "The loss at epoch 8 was 49.11886978149414 and the training accuracy is 0.9999995231628418\n",
            "The loss at epoch 9 was 42.670997619628906 and the training accuracy is 0.9989914894104004\n",
            "Unfrozen Blocks: 6, Current lr: 7.864320000000001e-05, Trainable Params: 53393314\n",
            "The loss at epoch 10 was 36.92530822753906 and the training accuracy is 0.9999995231628418\n",
            "The loss at epoch 11 was 32.21682357788086 and the training accuracy is 0.9999995231628418\n",
            "Unfrozen Blocks: 7, Current lr: 6.291456000000001e-05, Trainable Params: 60481186\n",
            "The loss at epoch 12 was 28.05002784729004 and the training accuracy is 0.9999995231628418\n",
            "The loss at epoch 13 was 24.506731033325195 and the training accuracy is 0.9999995231628418\n",
            "Unfrozen Blocks: 8, Current lr: 5.0331648000000016e-05, Trainable Params: 67569058\n",
            "The loss at epoch 14 was 21.76180076599121 and the training accuracy is 0.9999995231628418\n",
            "The loss at epoch 15 was 19.50817108154297 and the training accuracy is 0.9999995231628418\n",
            "Unfrozen Blocks: 9, Current lr: 4.026531840000002e-05, Trainable Params: 74656930\n",
            "The loss at epoch 16 was 17.532018661499023 and the training accuracy is 0.9999995231628418\n",
            "The loss at epoch 17 was 16.10764503479004 and the training accuracy is 0.9999995231628418\n",
            "Unfrozen Blocks: 10, Current lr: 3.221225472000002e-05, Trainable Params: 81744802\n",
            "The loss at epoch 18 was 14.732054710388184 and the training accuracy is 0.9999995231628418\n",
            "The loss at epoch 19 was 13.893786430358887 and the training accuracy is 0.9999995231628418\n",
            "Unfrozen Blocks: 11, Current lr: 2.5769803776000016e-05, Trainable Params: 88832674\n",
            "The loss at epoch 20 was 12.872425079345703 and the training accuracy is 0.9999995231628418\n",
            "The loss at epoch 21 was 12.161036491394043 and the training accuracy is 0.9999995231628418\n",
            "Unfrozen Blocks: 12, Current lr: 2.0615843020800013e-05, Trainable Params: 88832674\n",
            "The loss at epoch 22 was 11.55864429473877 and the training accuracy is 0.9999995231628418\n",
            "The loss at epoch 23 was 10.998501777648926 and the training accuracy is 0.9999995231628418\n",
            "Unfrozen Blocks: 13, Current lr: 1.649267441664001e-05, Trainable Params: 88832674\n",
            "The loss at epoch 24 was 10.615784645080566 and the training accuracy is 0.9999995231628418\n",
            "The loss at epoch 25 was 10.152776718139648 and the training accuracy is 0.9999995231628418\n",
            "Unfrozen Blocks: 14, Current lr: 1.319413953331201e-05, Trainable Params: 88832674\n",
            "The loss at epoch 26 was 9.895927429199219 and the training accuracy is 0.9999995231628418\n",
            "The loss at epoch 27 was 9.759913444519043 and the training accuracy is 0.9999995231628418\n",
            "Unfrozen Blocks: 15, Current lr: 1.0555311626649608e-05, Trainable Params: 88832674\n",
            "The loss at epoch 28 was 9.376128196716309 and the training accuracy is 0.9999995231628418\n",
            "The loss at epoch 29 was 9.29788589477539 and the training accuracy is 0.9999995231628418\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83B4OrvmZXur"
      },
      "source": [
        "###Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qU-PxoAZXus"
      },
      "source": [
        "def get_id(img_path):\n",
        "    camera_id = []\n",
        "    labels = []\n",
        "    for path, v in img_path:\n",
        "        label = path.split(\"/\")[-2]\n",
        "        filename = os.path.basename(path)\n",
        "        camera = filename.split('_')[0]\n",
        "        labels.append(int(label))\n",
        "        camera_id.append(int(camera))\n",
        "    return camera_id, labels"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8CAj5rvZXus"
      },
      "source": [
        "def extract_feature(model,dataloaders,device,image_dataset):\n",
        "    \"\"\"\n",
        "    Used to get the features/representation of queryset and galleryset\n",
        "    \"\"\"\n",
        "    features =  torch.FloatTensor()\n",
        "    count = 0\n",
        "    idx = 0\n",
        "    images = {}\n",
        "    for index,data in enumerate(dataloaders):\n",
        "        img, label = data    \n",
        "        img, label = img.to(device), label.to(device)\n",
        "\n",
        "        img_flip= torch.flip(img,[3])\n",
        "        output = model(img) # (B, D, H, W) --> B: batch size, HxWxD: feature volume size\n",
        "        output_flip = model(img_flip)\n",
        "        output = (output+output_flip)/2\n",
        "\n",
        "        n, c, h, w = img.size()\n",
        "        \n",
        "        count += n\n",
        "        features = torch.cat((features, output.detach().cpu()), 0)\n",
        "        idx += 1\n",
        "        images[index]=image_dataset.imgs[index][0]\n",
        "    return features,images\n",
        "\n",
        "def search(index,query: str, k=1):\n",
        "    \"\"\"\n",
        "    to do a similarity based search using faiss library\n",
        "    \"\"\"\n",
        "    encoded_query = query.unsqueeze(dim=0).numpy()\n",
        "    top_k = index.search(encoded_query, k)\n",
        "    return top_k"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw0hn-o-ZXut"
      },
      "source": [
        "def rank1(label, output):\n",
        "    if label==output[1][0][0]:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def rank5(label, output):\n",
        "    if label in output[1][0][:5]:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def calc_ap(label, output):\n",
        "    count = 0\n",
        "    score = 0\n",
        "    good = 0\n",
        "    for out in output[1][0]:\n",
        "        count += 1\n",
        "        if out==label:\n",
        "            good += 1            \n",
        "            score += (good/count)\n",
        "    if good==0:\n",
        "        return 0\n",
        "    return score/good"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXwei4n1ZXuu",
        "outputId": "a3b00f60-86d7-47db-8ce0-bcffdbddcdd2"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Using {}\".format(device))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35_DvvPTZXuu",
        "outputId": "84dc0655-7a06-410c-dd51-068074b9613d"
      },
      "source": [
        "import os\n",
        "!pip install faiss-gpu\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "import argparse\n",
        "!pip install timm\n",
        "import timm\n",
        "import torch\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.7/dist-packages (1.7.1.post3)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.7/dist-packages (0.4.12)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.19.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43IFMvr7ZXuv"
      },
      "source": [
        "wts_path = \"/content/drive/MyDrive/vision_project_data/ams_pool_sum_triplet_output.pth\"\n",
        "inp_path =\"/content/drive/MyDrive/vision_project_data/val\""
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Zt19rVjZXuw"
      },
      "source": [
        "class LATransformerTest_Pooldualsum(nn.Module):\n",
        "    def __init__(self, model, lmbd, class_num,part,num_blocks,int_dim ):\n",
        "        super(LATransformerTest_Pooldualsum, self).__init__()\n",
        "        \n",
        "        self.class_num = class_num\n",
        "        self.part = part # We cut the pool5 to sqrt(N) parts\n",
        "        self.num_blocks = num_blocks\n",
        "        self.model = model\n",
        "        self.model.head.requires_grad_ = False \n",
        "        self.cls_token = self.model.cls_token\n",
        "        self.pos_embed = self.model.pos_embed\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1,int_dim))\n",
        "        self.avgpool2 = nn.AdaptiveAvgPool2d((self.part,int_dim))\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.lmbd = lmbd\n",
        "        self.int_dim = int_dim\n",
        "#         for i in range(self.part):\n",
        "#             name = 'classifier'+str(i)\n",
        "#             setattr(self, name, ClassBlock(768, self.class_num, droprate=0.5, relu=False, bnorm=True, num_bottleneck=256))\n",
        "\n",
        "        \n",
        "\n",
        "    def forward(self,x):\n",
        "        \n",
        "        # Divide input image into patch embeddings and add position embeddings\n",
        "        x = self.model.patch_embed(x)\n",
        "        cls_token = self.cls_token.expand(x.shape[0], -1, -1) \n",
        "        x = torch.cat((cls_token, x), dim=1)\n",
        "        x = self.model.pos_drop(x + self.pos_embed)\n",
        "        \n",
        "        # Feed forward through transformer blocks\n",
        "        for i in range(self.num_blocks):\n",
        "            x = self.model.blocks[i](x)\n",
        "        x = self.model.norm(x)\n",
        "        \n",
        "        # extract the cls token\n",
        "        cls_token_out = x[:, 0].unsqueeze(1)\n",
        "        \n",
        "        # Average pool\n",
        "        x2= x.detach().clone()\n",
        "        x2 = self.avgpool2(x2)\n",
        "        x = torch.reshape(x[:,1:],(x.shape[0],self.part,self.part,self.int_dim))\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.reshape(x,(x.shape[0],self.part,self.int_dim))\n",
        "        x = (x+x2)/2\n",
        "        \n",
        "        # Add global cls token to each local token \n",
        "#         for i in range(self.part):\n",
        "#             out = torch.mul(x[:, i, :], self.lmbd)\n",
        "#             x[:,i,:] = torch.div(torch.add(cls_token_out.squeeze(),out), 1+self.lmbd)\n",
        "\n",
        "        return x.cpu()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R653wthcZXux",
        "outputId": "861f044a-4e89-44da-8ebd-e1672cb5fd03"
      },
      "source": [
        "\"\"\"\n",
        "Load saved model\n",
        "\"\"\"\n",
        "num_classes = 62\n",
        "vit_base = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=num_classes)\n",
        "vit_base= vit_base.to(device)\n",
        "\n",
        "# Create La-Transformer\n",
        "\n",
        "num_la_blocks = 14\n",
        "blocks = 12\n",
        "int_dim = 768\n",
        "lmbd = 8\n",
        "num_classes = 62\n",
        "model = LATransformerTest_Pooldualsum(vit_base, lmbd,num_classes,num_la_blocks,blocks,int_dim).to(device)\n",
        "model.load_state_dict(torch.load(wts_path), strict=False)\n",
        "model.eval()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LATransformerTest_Pooldualsum(\n",
              "  (model): VisionTransformer(\n",
              "    (patch_embed): PatchEmbed(\n",
              "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "      (norm): Identity()\n",
              "    )\n",
              "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
              "    (blocks): Sequential(\n",
              "      (0): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "    (pre_logits): Identity()\n",
              "    (head): Linear(in_features=768, out_features=62, bias=True)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 768))\n",
              "  (avgpool2): AdaptiveAvgPool2d(output_size=(14, 768))\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpvFK7YTZXuy"
      },
      "source": [
        "batch_size = 1"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtmfaPgSZXuy"
      },
      "source": [
        "transform_query_list = [\n",
        "    transforms.Resize((224,224), interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "]\n",
        "transform_gallery_list = [\n",
        "        transforms.Resize(size=(224,224), interpolation=transforms.InterpolationMode.BICUBIC), #Image.BICUBIC\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]\n",
        "\n",
        "data_transforms = {\n",
        "        'query': transforms.Compose( transform_query_list ),\n",
        "        'gallery': transforms.Compose(transform_gallery_list),\n",
        "    }\n",
        "\n",
        "\n",
        "image_datasets = {}\n",
        "\n",
        "image_datasets['query'] = datasets.ImageFolder(os.path.join(inp_path, 'query'),\n",
        "                                          data_transforms['query'])\n",
        "image_datasets['gallery'] = datasets.ImageFolder(os.path.join(inp_path, 'gallery'),\n",
        "                                          data_transforms['gallery'])\n",
        "query_loader = DataLoader(dataset = image_datasets['query'], batch_size=batch_size, shuffle=False )\n",
        "gallery_loader = DataLoader(dataset = image_datasets['gallery'], batch_size=batch_size, shuffle=False)\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3ZwrHqLZXuz"
      },
      "source": [
        "# Extract Query Features\n",
        "\n",
        "query_feature,query_images= extract_feature(model,query_loader,device,image_datasets['query'])\n",
        "\n",
        "# Extract Gallery Features\n",
        "\n",
        "gallery_feature,gallery_images = extract_feature(model,gallery_loader,device,image_datasets['gallery'])\n",
        "\n",
        "# Retrieve labels\n",
        "\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCcCOE86ZXuz"
      },
      "source": [
        "gallery_path = image_datasets['gallery'].imgs\n",
        "query_path = image_datasets['query'].imgs\n",
        "\n",
        "gallery_cam,gallery_label = get_id(gallery_path)\n",
        "query_cam,query_label = get_id(query_path)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTvm_p6hZXu0"
      },
      "source": [
        "concatenated_query_vectors = []\n",
        "for idx,query in enumerate(query_feature):\n",
        "    fnorm = torch.norm(query, p=2, dim=1, keepdim=True)*np.sqrt(14)\n",
        "    query_norm = query.div(fnorm.expand_as(query))\n",
        "    concatenated_query_vectors.append(query_norm.view((-1)))\n",
        "\n",
        "concatenated_gallery_vectors = []\n",
        "for idx,gallery in enumerate(gallery_feature):\n",
        "    fnorm = torch.norm(gallery, p=2, dim=1, keepdim=True)*np.sqrt(14)\n",
        "    gallery_norm = gallery.div(fnorm.expand_as(gallery))\n",
        "    concatenated_gallery_vectors.append(gallery_norm.view((-1)))"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LWjKzBQZXu0"
      },
      "source": [
        "index = faiss.IndexIDMap(faiss.IndexFlatIP(10752))\n",
        "index.add_with_ids(np.array([t.numpy() for t in concatenated_gallery_vectors]),np.array(gallery_label).astype(np.int64))\n",
        "\n",
        "index2 = (faiss.IndexFlatIP((14*768)))\n",
        "index2.add(np.array([t.numpy() for t in concatenated_gallery_vectors]))"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-_qAW1t0KAg"
      },
      "source": [
        "from shutil import copyfile\n",
        "import os\n",
        "vis_path = \"/content/drive/MyDrive/vision_project_data/visualisation\"\n",
        "mod_str = \"improved\""
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yn-PIsTzZXu1",
        "outputId": "74d5f6e0-b698-472d-b352-a56b8ba7811f"
      },
      "source": [
        "rank1_score = 0\n",
        "rank5_score = 0\n",
        "ap = 0\n",
        "itr_no = 0\n",
        "count = 0\n",
        "for query, label in zip(concatenated_query_vectors, query_label):\n",
        "    count +=1\n",
        "    label = label\n",
        "    output_met = search(index,query, k=10)\n",
        "\n",
        "    encoded_query = query.unsqueeze(dim=0).numpy()\n",
        "\n",
        "    output = index.search(encoded_query,10)\n",
        "    output2 = index2.search(encoded_query,10)\n",
        "    label_pred = output[1][0][0]\n",
        "\n",
        "    rank1_score += rank1(label, output_met) \n",
        "    rank5_score += rank5(label, output_met) \n",
        "    print(\"Correct: {}, Total: {}, Incorrect: {}\".format(rank1_score, count, count-rank1_score))\n",
        "    ap += calc_ap(label, output)\n",
        "\n",
        "    list_preds = output2[1]\n",
        "    pred_img = gallery_images[list_preds[0][0]]\n",
        "    pred_img2 = gallery_images[list_preds[0][1]]\n",
        "    pred_img3 = gallery_images[list_preds[0][2]]\n",
        "    pred_img4 = gallery_images[list_preds[0][3]]\n",
        "    pred_img5 = gallery_images[list_preds[0][4]]\n",
        "    pred_img6 = gallery_images[list_preds[0][5]]\n",
        "    pred_img7 = gallery_images[list_preds[0][6]]\n",
        "    pred_img8 = gallery_images[list_preds[0][7]]\n",
        "    pred_img9 = gallery_images[list_preds[0][8]]\n",
        "    pred_img10 = gallery_images[list_preds[0][9]]\n",
        "    inp_img = query_images[itr_no]\n",
        "    \n",
        "    if os.path.exists(vis_path+'/'+mod_str+'_'+str(count)):\n",
        "      shutil.rmtree(vis_path+'/'+mod_str+'_'+str(count))\n",
        "    os.mkdir(vis_path+'/'+mod_str+'_'+str(count))\n",
        "\n",
        "    copyfile(inp_img, vis_path+'/'+mod_str+'_'+str(count)+'/query'+'.png')\n",
        "    copyfile(pred_img, vis_path+'/'+mod_str+'_'+str(count)+'/gallery_'+'1'+'.png')\n",
        "    copyfile(pred_img2, vis_path+'/'+mod_str+'_'+str(count)+'/gallery_'+'2'+'.png')\n",
        "    copyfile(pred_img3, vis_path+'/'+mod_str+'_'+str(count)+'/gallery_'+'3'+'.png')\n",
        "    copyfile(pred_img4, vis_path+'/'+mod_str+'_'+str(count)+'/gallery_'+'4'+'.png')\n",
        "    copyfile(pred_img5, vis_path+'/'+mod_str+'_'+str(count)+'/gallery_'+'5'+'.png')\n",
        "    copyfile(pred_img6, vis_path+'/'+mod_str+'_'+str(count)+'/gallery_'+'6'+'.png')\n",
        "    copyfile(pred_img7, vis_path+'/'+mod_str+'_'+str(count)+'/gallery_'+'7'+'.png')\n",
        "    copyfile(pred_img8, vis_path+'/'+mod_str+'_'+str(count)+'/gallery_'+'8'+'.png')\n",
        "    copyfile(pred_img9, vis_path+'/'+mod_str+'_'+str(count)+'/gallery_'+'9'+'.png')\n",
        "    copyfile(pred_img10, vis_path+'/'+mod_str+'_'+str(count)+'/gallery_'+'10'+'.png')\n",
        "    itr_no +=1\n",
        "\n",
        "print(\"Rank1: %.3f, Rank5: %.3f, mAP: %.3f\"%(rank1_score/len(query_feature), \n",
        "                                              rank5_score/len(query_feature), \n",
        "                                              ap/len(query_feature)))  "
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct: 1, Total: 1, Incorrect: 0\n",
            "Correct: 2, Total: 2, Incorrect: 0\n",
            "Correct: 3, Total: 3, Incorrect: 0\n",
            "Correct: 4, Total: 4, Incorrect: 0\n",
            "Correct: 5, Total: 5, Incorrect: 0\n",
            "Correct: 6, Total: 6, Incorrect: 0\n",
            "Correct: 7, Total: 7, Incorrect: 0\n",
            "Correct: 8, Total: 8, Incorrect: 0\n",
            "Correct: 9, Total: 9, Incorrect: 0\n",
            "Correct: 10, Total: 10, Incorrect: 0\n",
            "Correct: 11, Total: 11, Incorrect: 0\n",
            "Correct: 12, Total: 12, Incorrect: 0\n",
            "Correct: 13, Total: 13, Incorrect: 0\n",
            "Correct: 14, Total: 14, Incorrect: 0\n",
            "Correct: 15, Total: 15, Incorrect: 0\n",
            "Correct: 16, Total: 16, Incorrect: 0\n",
            "Correct: 17, Total: 17, Incorrect: 0\n",
            "Correct: 18, Total: 18, Incorrect: 0\n",
            "Correct: 19, Total: 19, Incorrect: 0\n",
            "Correct: 20, Total: 20, Incorrect: 0\n",
            "Correct: 21, Total: 21, Incorrect: 0\n",
            "Correct: 22, Total: 22, Incorrect: 0\n",
            "Correct: 23, Total: 23, Incorrect: 0\n",
            "Correct: 24, Total: 24, Incorrect: 0\n",
            "Correct: 25, Total: 25, Incorrect: 0\n",
            "Correct: 26, Total: 26, Incorrect: 0\n",
            "Correct: 27, Total: 27, Incorrect: 0\n",
            "Correct: 28, Total: 28, Incorrect: 0\n",
            "Rank1: 1.000, Rank5: 1.000, mAP: 0.975\n"
          ]
        }
      ]
    }
  ]
}